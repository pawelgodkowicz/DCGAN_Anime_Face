{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCGAN - Anime Faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to generate images of anime faces using a Deep Convolutional Generative Adversarial Network (DCGAN). \n",
    "\n",
    "The dataset was downloaded from kaggle site: https://www.kaggle.com/soumikrakshit/anime-faces. \n",
    "\n",
    "Generative Adversarial Networks (GANs) are one of the most interesting ideas in computer science today. Two models are trained simultaneously by an adversarial process. A generator (\"the fraud\") learns to create images that look real, while a discriminator (\"judge\") learns to tell real images apart from fakes.\n",
    "\n",
    "During training, the generator progressively becomes better at creating images that look real, while the discriminator becomes better at telling them apart. The process reaches equilibrium when the discriminator can no longer distinguish real images from fakes.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Package importing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Reshape, Dropout, Dense, Flatten, LeakyReLU, Conv2D, Conv2DTranspose\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gs\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import imageio\n",
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Architecture and utilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the generator and discriminator are defined using the Keras Functional API. The generator uses Conv2D and Conv2DTranspose (upsampling) layers to produce an image from a seed (random noise). Start with a Dense layer that takes this seed as input, then upsample several times until you reach the desired image size of 64x64x3. The LeakyReLU activation function for each layer, except the output layer which uses tanh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator(noise_shape):\n",
    "    \n",
    "    #The Adam optimization algorithm\n",
    "    optimizer = Adam(lr=0.00015, beta_1=0.5)\n",
    "    \n",
    "    generator_input = Input(shape=(noise_shape))\n",
    "    \n",
    "    # fully conected layers\n",
    "    g = Dense(128 * 32 * 32)(generator_input) \n",
    "    # Leaky ReLU activation\n",
    "    g = LeakyReLU()(g) \n",
    "    g = Reshape((32, 32, 128))(g)\n",
    "    \n",
    "    # 2D convolution layer with 256 filters and kernel size 5\n",
    "    g = Conv2D(256, 5, padding='same')(g) \n",
    "    g = LeakyReLU()(g)\n",
    "    # Transposed convolution layer with 256 filters kernel size 4 and strides 2\n",
    "    g = Conv2DTranspose(256, 4, strides=2, padding='same')(g) \n",
    "    g = LeakyReLU()(g)\n",
    "\n",
    "    g = Conv2D(256, 5, padding='same')(g)\n",
    "    g = LeakyReLU()(g)\n",
    "    g = Conv2D(256, 5, padding='same')(g)\n",
    "    g = LeakyReLU()(g)\n",
    "\n",
    "    g = Conv2D(3, 7, activation='tanh', padding='same')(g)\n",
    "\n",
    "    generator_model = Model(generator_input, g)\n",
    "    generator_model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    print(20*'*' + ' Generator model '+ 20*'*')\n",
    "    generator_model.summary()\n",
    "\n",
    "    return generator_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The discriminator is a CNN-based image classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator(image_shape):\n",
    "    \n",
    "    optimizer = Adam(lr=0.0002, beta_1=0.5)\n",
    "\n",
    "    discriminator_input = Input(shape=image_shape)\n",
    "    d = Conv2D(128, 3)(discriminator_input)\n",
    "    d = LeakyReLU()(d)\n",
    "    \n",
    "    d = Conv2D(128, 4, strides=2)(d)\n",
    "    d = LeakyReLU()(d)\n",
    "    \n",
    "    d = Conv2D(128, 4, strides=2)(d)\n",
    "    d = LeakyReLU()(d)\n",
    "    \n",
    "    d = Conv2D(128, 4, strides=2)(d)\n",
    "    d = LeakyReLU()(d)\n",
    "    \n",
    "    # Flatten the input image\n",
    "    d = Flatten()(d)\n",
    "    #Droping out units to prevent over-fitting\n",
    "    d = Dropout(0.4)(d)\n",
    "    \n",
    "    # Output layer with sigmoid activation\n",
    "    d = Dense(1, activation='sigmoid')(d)\n",
    "\n",
    "    discriminator_model = Model(discriminator_input, d)\n",
    "    discriminator_model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    print(20*'*' + ' Discriminator model '+ 20*'*')\n",
    "    discriminator_model.summary()\n",
    "    \n",
    "    return discriminator_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_img(img):\n",
    "    '''Image normalisation to keep values between -1 and 1 for stability.'''\n",
    "    images = np.array(img)\n",
    "    images = (images / 127.5) - 1\n",
    "    images = images.astype('float32')\n",
    "    return images\n",
    "\n",
    "def denorm_img(img):\n",
    "    '''Image transformation to original.'''\n",
    "    img = (img + 1) * 127.5\n",
    "    return img.astype(np.uint8)\n",
    "\n",
    "def get_data():\n",
    "    '''Importing data from a working directory'''\n",
    "    all_images = []\n",
    "    for index, filename in enumerate(glob.glob(data_dir)):\n",
    "        image = imageio.imread(filename, as_gray=False, pilmode='RGB')\n",
    "        all_images.append(image)\n",
    "        \n",
    "    return norm_img(all_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DCGAN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gan(g, d):\n",
    "    optimizer = Adam(lr=0.00015, beta_1=0.5)\n",
    "    \n",
    "    # Combined Generator -> Discriminator model\n",
    "    generator_input = Input(shape=noise_shape)\n",
    "    gan_input = g(generator_input)\n",
    "    gan_output = d(gan_input)\n",
    "    \n",
    "    gan = Model(generator_input, gan_output)\n",
    "    gan.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    print(10*'*' + ' Generative Adversarial Networks Model '+ 10*'*'   )\n",
    "    gan.summary()\n",
    "    \n",
    "    return gan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model and data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************** Discriminator model ********************\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 64, 64, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 62, 62, 128)       3584      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 62, 62, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 30, 30, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 30, 30, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 6, 6, 128)         262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 4609      \n",
      "=================================================================\n",
      "Total params: 795,009\n",
      "Trainable params: 795,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "******************** Generator model ********************\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 1, 1, 100)]       0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1, 1, 131072)      13238272  \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 1, 1, 131072)      0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 32, 32, 256)       819456    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 64, 64, 256)       1048832   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 64, 64, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 64, 64, 256)       1638656   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 64, 64, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 64, 64, 256)       1638656   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 64, 64, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 64, 64, 3)         37635     \n",
      "=================================================================\n",
      "Total params: 18,421,507\n",
      "Trainable params: 18,421,507\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "********** Generative Adversarial Networks Model **********\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 1, 1, 100)]       0         \n",
      "_________________________________________________________________\n",
      "model_1 (Model)              (None, 64, 64, 3)         18421507  \n",
      "_________________________________________________________________\n",
      "model (Model)                (None, 1)                 795009    \n",
      "=================================================================\n",
      "Total params: 19,216,516\n",
      "Trainable params: 18,421,507\n",
      "Non-trainable params: 795,009\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "noise_shape = (1, 1, 100)\n",
    "image_shape = (64,64,3)\n",
    "\n",
    "discriminator = make_discriminator(image_shape=image_shape)\n",
    "generator = make_generator(noise_shape=noise_shape)\n",
    "# Keep Discriminatorâ€™s parameters constant for Generator training\n",
    "discriminator.trainable = False\n",
    "\n",
    "# Build and compile DCGAN model with fixed Discriminator to train the Generator\n",
    "gan = make_gan(generator, discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_dim = 100\n",
    "EPOCHS = 10000\n",
    "batch_size = 128\n",
    "\n",
    "# data_dir = 'D:/DATASET/animefaces/*.*'\n",
    "# save_images_dir = 'D:/projekt_output/Images/'\n",
    "# save_model_dir = 'D:/projekt_output/Model/'\n",
    "# save_gif_dir = 'D:/projekt_output/Gif/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_fake_losses = []\n",
    "discriminator_real_losses = []\n",
    "gan_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'D:/DATASET/animefaces/*.*'\n",
    "save_images_dir = 'D:/projekt_output/Images2/'\n",
    "chosen_face_dir = 'D:/projekt_output/One_face/'\n",
    "save_model_dir = 'D:/projekt_output/Model2/'\n",
    "save_gif_dir = 'D:/projekt_output/Gif2/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n",
    "The training loop begins with generator receiving a random seed as input. That seed is used to produce an image. The discriminator is then used to classify real images and fakes images (produced by the generator). The loss is calculated for each of these models, and the gradients are used to update the generator and discriminator.\n",
    "\n",
    "We use tqdm library to display an extensible progress bar and run it for 10 000 epochs. We can see, the model learning process took 12 hours and 40 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ece122215dcc4cc8870c7047669bf213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10000\n",
      "Discriminator: real loss: 0.687637 fake loss: 0.695758\n",
      "GAN loss: 0.691201\n",
      "Epoch: 1000/10000\n",
      "Discriminator: real loss: 0.665380 fake loss: 0.692636\n",
      "GAN loss: 0.873625\n",
      "Epoch: 2000/10000\n",
      "Discriminator: real loss: 0.687514 fake loss: 0.644818\n",
      "GAN loss: 1.000669\n",
      "Epoch: 3000/10000\n",
      "Discriminator: real loss: 0.671620 fake loss: 0.731917\n",
      "GAN loss: 0.769045\n",
      "Epoch: 4000/10000\n",
      "Discriminator: real loss: 0.698418 fake loss: 0.708435\n",
      "GAN loss: 0.756048\n",
      "Epoch: 5000/10000\n",
      "Discriminator: real loss: 0.706463 fake loss: 0.683296\n",
      "GAN loss: 0.766010\n",
      "Epoch: 6000/10000\n",
      "Discriminator: real loss: 0.689649 fake loss: 0.699485\n",
      "GAN loss: 0.738321\n",
      "Epoch: 7000/10000\n",
      "Discriminator: real loss: 0.709491 fake loss: 0.672267\n",
      "GAN loss: 0.747225\n",
      "Epoch: 8000/10000\n",
      "Discriminator: real loss: 0.693481 fake loss: 0.707716\n",
      "GAN loss: 0.717819\n",
      "Epoch: 9000/10000\n",
      "Discriminator: real loss: 0.701227 fake loss: 0.682146\n",
      "GAN loss: 0.742111\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chosen_noise = np.random.normal(0, 1, size=(batch_size,) + noise_shape)\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "\n",
    "#     print(f\"Epoch: {epoch}/{EPOCHS}\")\n",
    "    \n",
    "    # Get a random batch of real images\n",
    "    idx = np.random.randint(0, X.shape[0], batch_size)\n",
    "    real_data_X = X[idx]\n",
    "    \n",
    "    # Generate a batch of fake images\n",
    "    noise = np.random.normal(0, 1, size=(batch_size,) + noise_shape)\n",
    "    fake_data_X = generator.predict(noise)\n",
    "    \n",
    "    if (epoch % 50) == 0:\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        gs1 = gs.GridSpec(8, 8)\n",
    "        gs1.update(wspace=0, hspace=0)\n",
    "        random_indices = np.random.choice(fake_data_X.shape[0], 64, replace=False)\n",
    "        for i in range(64):\n",
    "            ax1 = plt.subplot(gs1[i])\n",
    "            ax1.set_aspect('equal')\n",
    "            random_index = random_indices[i]\n",
    "            image = fake_data_X[random_index, :, :, :]\n",
    "            fig = plt.imshow(denorm_img(image))\n",
    "            plt.axis('off')\n",
    "            fig.axes.get_xaxis().set_visible(False)\n",
    "            fig.axes.get_yaxis().set_visible(False)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_images_dir + str(epoch).zfill(5) + \"_image.png\", bbox_inches='tight', pad_inches=0)\n",
    "        plt.close()\n",
    "        \n",
    "        \n",
    "\n",
    "    # concatenate real and fake data samples\n",
    "    data_X = np.concatenate([real_data_X, fake_data_X])\n",
    "    #add noise to the label inputs\n",
    "    real_data_Y = np.ones(batch_size) - np.random.random_sample(batch_size) * 0.2\n",
    "    fake_data_Y = np.random.random_sample(batch_size) * 0.2\n",
    "\n",
    "    data_Y = np.concatenate((real_data_Y, fake_data_Y))\n",
    "\n",
    "    discriminator.trainable = True\n",
    "    generator.trainable = False\n",
    "    \n",
    "    ''' Training the Discriminator'''\n",
    "    \n",
    "    dis_metrics_real = discriminator.train_on_batch(real_data_X, real_data_Y)\n",
    "    dis_metrics_fake = discriminator.train_on_batch(fake_data_X, fake_data_Y)\n",
    "\n",
    "#     if ((epoch+1) % 1000) == 0:\n",
    "#         print(f\"Epoch: {epoch}/{EPOCHS}\")\n",
    "#         print(\"Discriminator: real loss: %f fake loss: %f\" % (dis_metrics_real[0], dis_metrics_fake[0]))\n",
    "\n",
    "    discriminator_fake_losses.append(dis_metrics_fake[0])\n",
    "    discriminator_real_losses.append(dis_metrics_real[0])\n",
    "    \n",
    "    ''' Training the Generator '''\n",
    "    \n",
    "    generator.trainable = True\n",
    "\n",
    "    GAN_X = np.random.normal(0, 1, size=(batch_size,) + noise_shape)\n",
    "\n",
    "    GAN_Y = real_data_Y\n",
    "\n",
    "    discriminator.trainable = False\n",
    "\n",
    "    gan_metrics = gan.train_on_batch(GAN_X, GAN_Y)\n",
    "    \n",
    "    if (epoch % 1000) == 0:\n",
    "        print(f\"Epoch: {epoch}/{EPOCHS}\")\n",
    "        print(\"Discriminator: real loss: %f fake loss: %f\" % (dis_metrics_real[0], dis_metrics_fake[0]))\n",
    "        print(\"GAN loss: %f\" % (gan_metrics[0]))\n",
    "\n",
    "    text_file = open(save_model_dir + \"\\\\training_log.txt\", \"a\")\n",
    "    text_file.write(\"Epoch: %d Discriminator: real loss: %f fake loss: %f GAN loss: %f\\n\" % (epoch, \n",
    "                                                                                             dis_metrics_real[0],\n",
    "                                                                                             dis_metrics_fake[0], \n",
    "                                                                                             gan_metrics[0]))\n",
    "    text_file.close()\n",
    "    gan_losses.append(gan_metrics[0])\n",
    "    \n",
    "    if ((epoch + 1) % 500) == 0:\n",
    "        discriminator.trainable = True\n",
    "        generator.trainable = True\n",
    "        generator.save(save_model_dir + str(epoch) + \"_GENERATOR.hdf5\")\n",
    "        discriminator.save(save_model_dir + str(epoch) + \"_DISCRIMINATOR.hdf5\")\n",
    "        \n",
    "        picture = generator.predict(chosen_noise)\n",
    "        plt.figure(figsize=(2,2))\n",
    "        chosen_image = picture[15, :, :, :]\n",
    "        fig = plt.imshow(denorm_img(chosen_image))\n",
    "        plt.axis('off')\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(chosen_face_dir + str(epoch) + \"_image.png\", bbox_inches='tight', pad_inches=0)\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Exporting the results to csv'''\n",
    "\n",
    "df = pd.DataFrame({\n",
    "        'epochs': list(range(EPOCHS)),\n",
    "        'disc_fake_loss': discriminator_fake_losses,\n",
    "        'disc_real_loss': discriminator_real_losses,\n",
    "        'gan_loss': gan_losses\n",
    "    }).set_index('epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mean_disc_loss'] = (df.disc_fake_loss + df.disc_real_loss) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(save_model_dir+'losses.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorflowGPU",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
